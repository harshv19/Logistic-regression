{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_data(path_of_folders):\n",
    "    list_of_images=[]\n",
    "    classes=[]\n",
    "    list_of_folder_names = os.listdir(path_of_folders)\n",
    "    k=0\n",
    "    for each_folder in list_of_folder_names:\n",
    "        base_path=os.path.join(path_of_folders,each_folder)\n",
    "        list_of_image_in_folder=os.listdir(base_path)\n",
    "        list_of_images.extend(map(lambda x: plt.imread(os.path.join(base_path,x)).reshape(1024,),list_of_image_in_folder))\n",
    "        classes.extend(map(lambda x: k,list_of_image_in_folder))\n",
    "        k+=1\n",
    "    images_df = pd.DataFrame(np.array(list_of_images))\n",
    "    classes = np.array(classes)\n",
    "    return (images_df,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(data,params):\n",
    "    ans = np.exp(np.matmul(data,params[0])+params[1])\n",
    "    ans = (ans/(np.sum(ans,axis=1)).reshape(data.shape[0],1))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neg_log_loss(labels,softmax,theta,regularisation,lamb):\n",
    "    ans = -np.mean(labels*np.log(softmax))\n",
    "    if regularisation:\n",
    "        ans = ans - lamb/(2*softmax.shape[0])*(np.linalg.norm(theta))\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdash_thet0(sftmax,labels):\n",
    "    '''derivative of theta_0 is calculated as summation of labels-Softmax'''\n",
    "    ans = np.sum(sftmax-labels,axis=0)\n",
    "    return np.array(ans)/sftmax.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdash_thet(data,labels,sftmx,theta,regularisation,lamb):\n",
    "    '''derivative of theta is calculated as (Updated_labels.T x Data).T\n",
    "       Updated labels = -(labels-Softmax)\n",
    "       This negative sign is due to Negative Log Loss'''\n",
    "    ans = np.matmul(np.transpose(data),sftmx-labels)\n",
    "    if regularisation:\n",
    "        ans = np.matmul(np.transpose(data),sftmx-labels) - lamb/(2*data.shape[0])*np.sum(theta)\n",
    "    return np.array(ans)/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(training_data,labels,thetas,epsilon,learning_rate,Type=None,epochs=0,batch_size=0,regularisation=None,lamb=0):\n",
    "    '''\n",
    "    This function will use Batch Gradient Descent and Mini-Batch Gradient Descent \n",
    "    for optimisation depending upon the choice of user.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_data: array-like\n",
    "        input of shape (m,n) where m represents the number of training samples.\n",
    "    \n",
    "    labels: array-like\n",
    "        input of shape (m,1)\n",
    "        should not be one hot encoded\n",
    "    \n",
    "    thetas: list of arrays [thetas,theta_0]\n",
    "        they should be pre-initialised\n",
    "        thetas should be of the shape (n,k) where k is the number of unique labels\n",
    "        theta_0 should be of the shape (1,k)\n",
    "    \n",
    "    epsilon: float\n",
    "        error tolerance\n",
    "    \n",
    "    learning_rate: float\n",
    "    \n",
    "    Type: str input, optional\n",
    "        use 'mini' for Mini-Batch Gradient Descent\n",
    "        use None for Batch Gradient Descent\n",
    "    \n",
    "    epochs: int, optional\n",
    "        should be used if user has mentioned the type of gradient descent to be used\n",
    "        \n",
    "    batch_size: int, optional\n",
    "        size of the mini batch\n",
    "        \n",
    "    regularisation: bool, optional\n",
    "        Default None\n",
    "        set to True for l2 regularisation\n",
    "        \n",
    "    lamb: int, optional\n",
    "        a positive value which will be used as regularisation parameter\n",
    "        \n",
    "        \n",
    "    returns a tuple containing thet_f, thet0_f and neg_loss_history\n",
    "    '''\n",
    "    assert thetas[0].shape==(training_data.shape[1],len(np.unique(labels)))\n",
    "    assert thetas[1].shape==(1,len(np.unique(labels)))\n",
    "    assert epochs>=0\n",
    "    assert learning_rate>=0\n",
    "    assert epsilon>=0\n",
    "    assert lamb>=0\n",
    "    assert batch_size>=0 and batch_size<training_data.shape[0]\n",
    "    \n",
    "    \n",
    "    thet_i = thetas[0]\n",
    "    thet0_i = thetas[1]\n",
    "    \n",
    "    iterations=[]\n",
    "    neg_log_history=[]\n",
    "    deriv=[]\n",
    "    iden=np.identity(len(np.unique(labels)))\n",
    "    classes_train = labels\n",
    "        \n",
    "    if Type==None:\n",
    "        training_data_1 = training_data\n",
    "        classes_tr = np.array(iden[classes_train])\n",
    "        i=0\n",
    "        while True:\n",
    "            sftmx_ini = Softmax(training_data,[thet_i,thet0_i])\n",
    "            #print('labels',labels.shape)\n",
    "\n",
    "\n",
    "            thet0_f = thet0_i - learning_rate*(fdash_thet0(sftmx_ini,classes_tr))\n",
    "            #print('thet0_f',thet0_i.shape)\n",
    "            thet_f = thet_i - learning_rate*(fdash_thet(training_data_1,classes_tr,sftmx_ini,theta=thet_i,regularisation=regularisation,lamb=lamb))\n",
    "            #print('thet_f',thet_f.shape)\n",
    "\n",
    "            sftmx_fin = Softmax(training_data_1,[thet_f,thet0_f])\n",
    "\n",
    "\n",
    "            neg_loss = abs(Neg_log_loss(classes_tr,sftmx_fin,thet_f,regularisation,lamb) - Neg_log_loss(classes_tr,sftmx_ini,thet_i,regularisation,lamb))\n",
    "            neg_log_history.append(abs(Neg_log_loss(classes_tr,sftmx_fin,thet_f,regularisation,lamb)))\n",
    "            iterations.append(i)\n",
    "            deriv.append(thet_f[:,0])\n",
    "            if i%500==0:\n",
    "                print('neg log loss at iteration {} is {} {}'.format(i,neg_log_history[-1],neg_loss))\n",
    "\n",
    "            i+=1\n",
    "\n",
    "            if (neg_loss)<epsilon:\n",
    "                print('neg log loss final at iteration {} is {}'.format(i,neg_log_history[-1]))\n",
    "                return thet_f,thet0_f,neg_log_history,deriv\n",
    "            else:\n",
    "                thet0_i = thet0_f\n",
    "                thet_i = thet_f\n",
    "\n",
    "                \n",
    "    elif Type=='mini':\n",
    "        #combine data and labels\n",
    "        shuffled_data = pd.concat([pd.DataFrame(cp.asnumpy(training_data)),pd.DataFrame((classes_train))],axis=1)\n",
    "        shuffled_data = shuffled_data.sample(frac=1)\n",
    "        \n",
    "        #shuffle the data\n",
    "        training_data,classes_train = np.array(shuffled_data.iloc[:,0:training_data.shape[1]]),np.array(shuffled_data.iloc[:,-1])\n",
    "\n",
    "        classes_train=classes_train.reshape(training_data.shape[0])\n",
    "        batches = training_data.shape[0]//batch_size\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            for batch in range(batches):\n",
    "                training_data_1 = np.array(training_data[batch*batch_size:(batch+1)*batch_size])\n",
    "                classes_tr = np.array(iden[classes_train[batch*batch_size:(batch+1)*batch_size]])\n",
    "                \n",
    "                sftmx_ini = Softmax(training_data_1,[thet_i,thet0_i])\n",
    "\n",
    "                thet0_f = thet0_i - learning_rate*(fdash_thet0(sftmx_ini,classes_tr))\n",
    "                thet_f = thet_i - learning_rate*(fdash_thet(training_data_1,classes_tr,sftmx_ini,thet_i,regularisation,lamb))\n",
    "\n",
    "                sftmx_fin = Softmax(training_data_1,[thet_f,thet0_f])\n",
    "                \n",
    "\n",
    "                neg_loss = abs(Neg_log_loss(classes_tr,sftmx_fin,thet_f,regularisation,lamb) - Neg_log_loss(classes_tr,sftmx_ini,thet_i,regularisation,lamb))\n",
    "                neg_log_history.append(abs(Neg_log_loss(classes_tr,sftmx_fin,thet_f,regularisation,lamb)))\n",
    "                iterations.append((e,batch))\n",
    "\n",
    "\n",
    "#                 if (neg_loss)<epsilon:\n",
    "#                     print('neg log loss final at iteration {} {} is {}'.format(e,batch,neg_loss))\n",
    "# #                     return thet_f,thet0_f,neg_log_history,deriv\n",
    "#                 else:\n",
    "                thet0_i = thet0_f\n",
    "                thet_i = thet_f\n",
    "            if e%10==0:\n",
    "                print('neg log loss after epoch {} is {} {}'.format(e,neg_log_history[-1],neg_loss))\n",
    "            deriv.append(thet_f[:,0])\n",
    "\n",
    "        print('neg log loss after epoch {} is {} {}'.format(e,neg_log_history[-1],neg_loss))\n",
    "    return thet_f,thet0_f,neg_log_history,deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "path_of_train_folders = 'D:/Datasets/DevanagariHandwrittenCharacterDataset/Train'\n",
    "raw_data,classes_train = Load_data(path_of_train_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing data\n",
    "path_of_test_folders = 'D:/Datasets/DevanagariHandwrittenCharacterDataset/Test'\n",
    "test_data,classes_test = Load_data(path_of_test_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a multi-class LR problem, we will create one-hot-encoded vectors as our labels. This makes it easier to apply Softmax function for probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iden=np.identity(46)\n",
    "classes_tr = iden[classes_train]\n",
    "classes_tr = np.array(classes_tr)\n",
    "raw_data_copy = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 1024)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA for dimensionality reduction. This will reduce the amount of calculations to be done and will\n",
    "#improve the execution speed.\n",
    "cov_matrix = raw_data_copy.cov()\n",
    "q,lam,qt = np.linalg.svd(cov_matrix)\n",
    "trace = np.sum(lam)\n",
    "f_vector=[]\n",
    "s=0\n",
    "for i in range(len(lam)):\n",
    "    s+=lam[i]\n",
    "    if s/trace>0.97:\n",
    "        break\n",
    "    else:\n",
    "        f_vector.append(q[:,i])\n",
    "f_vector = np.array(f_vector)\n",
    "f_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78200, 238)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = np.matmul(cp.array(raw_data_copy),np.transpose(f_vector))\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg log loss after epoch 0 is 0.07927907947742127 3.112476981098622e-05\n",
      "neg log loss after epoch 10 is 0.053378486373530286 2.447947030378289e-05\n",
      "neg log loss after epoch 20 is 0.04173628972844582 2.0219167498697344e-05\n",
      "neg log loss after epoch 30 is 0.03547434704548097 1.7802997379627594e-05\n",
      "neg log loss after epoch 40 is 0.03148643480833062 1.6364560580003906e-05\n",
      "neg log loss after epoch 50 is 0.028644195484804805 1.5425132573316375e-05\n",
      "neg log loss after epoch 60 is 0.02646291833036317 1.4757293376920638e-05\n",
      "neg log loss after epoch 70 is 0.024702666412148067 1.4252009725115866e-05\n",
      "neg log loss after epoch 80 is 0.023230818123721644 1.3852637108013877e-05\n",
      "neg log loss after epoch 90 is 0.021967605424858018 1.3526968195815003e-05\n",
      "neg log loss after epoch 100 is 0.02086181118563573 1.325517629501699e-05\n",
      "neg log loss after epoch 110 is 0.019878810433762602 1.3024247615255058e-05\n",
      "neg log loss after epoch 120 is 0.018994200084621823 1.2825200439235684e-05\n",
      "neg log loss after epoch 130 is 0.018190180316758522 1.26515880166031e-05\n",
      "neg log loss after epoch 140 is 0.01745338807751988 1.2498637139715663e-05\n",
      "neg log loss after epoch 150 is 0.016773542147557663 1.23627237403022e-05\n",
      "neg log loss after epoch 160 is 0.016142563966772154 1.2241038417130229e-05\n",
      "neg log loss after epoch 170 is 0.015553988845683334 1.2131364777944256e-05\n",
      "neg log loss after epoch 180 is 0.01500256064420245 1.2031927817268348e-05\n",
      "neg log loss after epoch 190 is 0.014483945895902402 1.1941287404212964e-05\n",
      "neg log loss after epoch 199 is 0.01404224657551291 1.186625093330812e-05\n"
     ]
    }
   ],
   "source": [
    "#Run the training process\n",
    "thet0_i = np.zeros(shape=(1,46))\n",
    "thet_i = np.zeros(shape=(training_data.shape[1],46))\n",
    "\n",
    "tf,t0f,nlh,derivs = gradient_descent(training_data=training_data,labels=classes_train,thetas=[thet_i,thet0_i],\n",
    "                                     epsilon=1e-6,learning_rate=0.001,Type='mini',epochs=200,batch_size=170,\n",
    "                                     regularisation=True,lamb=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7037468030690537"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training accuracy\n",
    "ans=[]\n",
    "sft=Softmax(training_data,[tf,t0f])\n",
    "for i in range(training_data.shape[0]):\n",
    "    ans.append(np.argmax(sft[i]))\n",
    "np.count_nonzero(ans==classes_train)/training_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "classes_te = np.array(iden[classes_test])\n",
    "test_data_copy = test_data.copy()\n",
    "test_data_copy = np.matmul(np.array(test_data_copy),np.transpose(f_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.702463768115942"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing accuracy\n",
    "test_sft=Softmax(test_data_copy,[tf,t0f])\n",
    "ans_test = []\n",
    "for i in range(test_data_copy.shape[0]):\n",
    "    ans_test.append(np.argmax(test_sft[i]))\n",
    "np.count_nonzero(ans_test==classes_test)/test_data_copy.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy can be improved by tuning our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
